{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Plagiarism Detection - Phase 1: Indexing and Data Preparation\n",
    "This notebook handles all data preparation for the plagiarism detection system:\n",
    "\n",
    "Collects Python code from 5+ GitHub repositories\n",
    "Extracts individual functions from source files\n",
    "Builds FAISS semantic index using CodeBERT embeddings\n",
    "Builds BM25 lexical index for hybrid retrieval\n",
    "Creates labeled test dataset with 30+ examples (positive and negative cases)\n",
    "Saves all indexes and datasets for use by other notebooks\n",
    "\n",
    "Security Note: GitHub token is loaded from environment variables (not hardcoded) to comply with security requirements.\n",
    "Required environment variable: GITHUB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Embedding and search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# GitHub access\n",
    "from github import Github\n",
    "import requests\n",
    "\n",
    "# GitHub authentication\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"GitHub token not found! Set GITHUB_TOKEN as an environment variable.\")\n",
    "\n",
    "# Headers for authenticated requests\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "# Authenticate PyGithub (optional, if you use PyGithub library)\n",
    "g = Github(GITHUB_TOKEN)\n",
    "\n",
    "\n",
    "# Setup paths\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "CORPUS_DIR = DATA_DIR / 'reference_corpus'\n",
    "INDEX_DIR = BASE_DIR / 'indexes'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_DIR, CORPUS_DIR, INDEX_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection from GitHub\n",
    "This section collects Python code from 5 GitHub repositories, meeting the minimum requirement.\n",
    "\n",
    "Key Features:\n",
    "- Recursive traversal of repository directories\n",
    "- Filters out test files and system directories\n",
    "- Authenticates with GitHub API to avoid rate limits\n",
    "- Saves raw corpus with metadata (repo, path, URL) for traceability\n",
    "\n",
    "Output: data/reference_corpus/raw_corpus.json (150 Python files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading from TheAlgorithms/Python...\n",
      "  Downloaded 50 files\n",
      "\n",
      "Downloading from karan/Projects-Solutions...\n",
      "  Downloaded 0 files\n",
      "\n",
      "Downloading from geekcomputers/Python...\n",
      "  Downloaded 50 files\n",
      "\n",
      "Downloading from zhiwehu/Python-programming-exercises...\n",
      "  Downloaded 0 files\n",
      "\n",
      "Downloading from trekhleb/learn-python...\n",
      "  Downloaded 50 files\n",
      "\n",
      "‚úì Total files collected: 150\n",
      "‚úì Saved to data\\reference_corpus\\raw_corpus.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selected repositories - popular Python projects with diverse functionality\n",
    "REPOSITORIES = [\n",
    "    \"TheAlgorithms/Python\",              # Algorithm implementations (sorting, searching, etc.)\n",
    "    \"karan/Projects-Solutions\",          # Common programming project solutions\n",
    "    \"geekcomputers/Python\",              # Simple Python scripts and projects\n",
    "    \"zhiwehu/Python-programming-exercises\",  # Programming exercises with solutions\n",
    "    \"trekhleb/learn-python\"              # Python learning examples\n",
    "]\n",
    "\n",
    "def download_repository_code(repo_name: str, max_files: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Download Python files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_name: GitHub repository in format 'owner/repo'\n",
    "        max_files: Maximum number of files to download\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file information\n",
    "    \"\"\"\n",
    "    print(f\"\\nDownloading from {repo_name}...\")\n",
    "    \n",
    "    # Use GitHub API\n",
    "    base_url = f\"https://api.github.com/repos/{repo_name}/contents\"\n",
    "    files_data = []\n",
    "    \n",
    "    def fetch_python_files(url: str, path: str = \"\"):\n",
    "        \"\"\"Recursively fetch Python files from repository.\"\"\"\n",
    "        if len(files_data) >= max_files:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            response.raise_for_status()\n",
    "            contents = response.json()\n",
    "            \n",
    "            for item in contents:\n",
    "                if len(files_data) >= max_files:\n",
    "                    break\n",
    "                \n",
    "                if item['type'] == 'file' and item['name'].endswith('.py'):\n",
    "                    # Download file content\n",
    "                    file_response = requests.get(item['download_url'], headers=HEADERS)\n",
    "                    if file_response.status_code == 200:\n",
    "                        files_data.append({\n",
    "                            'repo': repo_name,\n",
    "                            'path': item['path'],\n",
    "                            'content': file_response.text,\n",
    "                            'url': item['html_url']\n",
    "                        })\n",
    "                        \n",
    "                elif item['type'] == 'dir' and not any(skip in item['path'] for skip in ['test', 'tests', '__pycache__', '.git']):\n",
    "                    # Recursively explore subdirectories (avoid test directories)\n",
    "                    fetch_python_files(item['url'], item['path'])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    fetch_python_files(base_url)\n",
    "    print(f\"  Downloaded {len(files_data)} files\")\n",
    "    return files_data\n",
    "\n",
    "# Download code from all repositories\n",
    "all_files = []\n",
    "for repo in REPOSITORIES:\n",
    "    repo_files = download_repository_code(repo)\n",
    "    all_files.extend(repo_files)\n",
    "\n",
    "print(f\"\\n‚úì Total files collected: {len(all_files)}\")\n",
    "\n",
    "# Save raw corpus\n",
    "corpus_file = CORPUS_DIR / 'raw_corpus.json'\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_files, f, indent=2)\n",
    "print(f\"‚úì Saved to {corpus_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code Chunking: Extract Functions\n",
    "Uses Python's AST module to parse source code and extract individual functions.\n",
    "\n",
    "Chunking Decision: Function-level (not file-level)\n",
    "- Rationale: Plagiarism typically occurs at function level, not entire files\n",
    "- Filters: Excludes trivial functions (< 50 characters)\n",
    "- Metadata: Preserves function name, docstring, line numbers, and source repository\n",
    "\n",
    "Each function gets unique ID: \"repo::filepath::function_name\"\n",
    "\n",
    "Output: data/reference_corpus/functions_corpus.json (399 functions across 3 repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting functions from code files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff63eb9d74564228b9825d936e693d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Extracted 399 functions\n",
      "‚úì Saved to data\\reference_corpus\\functions_corpus.json\n",
      "\n",
      "Corpus Statistics:\n",
      "repo\n",
      "TheAlgorithms/Python      96\n",
      "geekcomputers/Python     182\n",
      "trekhleb/learn-python    121\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def extract_functions_from_code(code: str, file_path: str, repo: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract individual functions from Python code.\n",
    "    \n",
    "    Args:\n",
    "        code: Python source code\n",
    "        file_path: Path to the file\n",
    "        repo: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of function dictionaries\n",
    "    \"\"\"\n",
    "    functions = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                # Extract function source code\n",
    "                function_lines = code.split('\\n')[node.lineno-1:node.end_lineno]\n",
    "                function_code = '\\n'.join(function_lines)\n",
    "                \n",
    "                # Skip very short functions (likely trivial)\n",
    "                if len(function_code.strip()) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # Extract docstring if present\n",
    "                docstring = ast.get_docstring(node) or \"\"\n",
    "                \n",
    "                functions.append({\n",
    "                    'id': f\"{repo}::{file_path}::{node.name}\",\n",
    "                    'repo': repo,\n",
    "                    'file': file_path,\n",
    "                    'name': node.name,\n",
    "                    'code': function_code,\n",
    "                    'docstring': docstring,\n",
    "                    'line_start': node.lineno,\n",
    "                    'line_end': node.end_lineno\n",
    "                })\n",
    "    except SyntaxError:\n",
    "        # Skip files with syntax errors\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        # Skip files with other parsing errors\n",
    "        pass\n",
    "    \n",
    "    return functions\n",
    "\n",
    "# Extract all functions from corpus\n",
    "print(\"Extracting functions from code files...\")\n",
    "all_functions = []\n",
    "\n",
    "for file_data in tqdm(all_files):\n",
    "    functions = extract_functions_from_code(\n",
    "        file_data['content'],\n",
    "        file_data['path'],\n",
    "        file_data['repo']\n",
    "    )\n",
    "    all_functions.extend(functions)\n",
    "\n",
    "print(f\"\\n‚úì Extracted {len(all_functions)} functions\")\n",
    "\n",
    "# Save function corpus\n",
    "functions_file = CORPUS_DIR / 'functions_corpus.json'\n",
    "with open(functions_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_functions, f, indent=2)\n",
    "print(f\"‚úì Saved to {functions_file}\")\n",
    "\n",
    "# Display statistics\n",
    "df_stats = pd.DataFrame(all_functions)\n",
    "print(\"\\nCorpus Statistics:\")\n",
    "print(df_stats.groupby('repo').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Embedding Index\n",
    "SEMANTIC EMBEDDING INDEX (Dense Retrieval)\n",
    "\n",
    "Builds FAISS index for pure embedding search and RAG systems (Homework Phase 2).\n",
    "\n",
    "Model: CodeBERT (microsoft/codebert-base)\n",
    "- Pre-trained on code from GitHub\n",
    "- Understands programming semantics and structure\n",
    "- Embedding dimension: 768\n",
    "\n",
    "Input Representation: Concatenation of function name, docstring, and code\n",
    "- Captures both semantic meaning and syntactic structure\n",
    "\n",
    "Index Type: FAISS IndexFlatIP (exact cosine similarity)\n",
    "- Normalized L2 vectors ‚Üí inner product = cosine similarity\n",
    "- Will be used for: detect_embedding(), detect_rag(), detect_hybrid_rag()\n",
    "\n",
    "Output Files:\n",
    "- indexes/faiss_index.bin (searchable index)\n",
    "- indexes/embeddings.npy (raw embeddings)\n",
    "- indexes/function_metadata.pkl (function details for retrieval results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded\n",
      "\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3e3cd6be31462db7c5d77bf7c47e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated embeddings shape: (399, 768)\n",
      "\n",
      "Building FAISS index...\n",
      "‚úì Index built with 399 vectors\n",
      "‚úì Saved FAISS index and metadata\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('microsoft/codebert-base')\n",
    "print(\"‚úì Model loaded\")\n",
    "\n",
    "# Generate embeddings for all functions\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "function_texts = [f\"{func['name']}\\n{func['docstring']}\\n{func['code']}\" for func in all_functions]\n",
    "embeddings = embedding_model.encode(\n",
    "    function_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "print(f\"‚úì Generated embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Build FAISS index\n",
    "print(\"\\nBuilding FAISS index...\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity for normalized vectors\n",
    "index.add(embeddings)\n",
    "print(f\"‚úì Index built with {index.ntotal} vectors\")\n",
    "\n",
    "# Save index and metadata\n",
    "faiss.write_index(index, str(INDEX_DIR / 'faiss_index.bin'))\n",
    "np.save(INDEX_DIR / 'embeddings.npy', embeddings)\n",
    "with open(INDEX_DIR / 'function_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(all_functions, f)\n",
    "\n",
    "print(\"‚úì Saved FAISS index and metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build BM25 Index\n",
    "LEXICAL INDEX (Sparse Retrieval - BM25)\n",
    "\n",
    "Builds BM25 index for hybrid RAG system (Homework Phase 2).\n",
    "\n",
    "Why BM25?\n",
    "- Captures exact lexical matches (variable names, keywords)\n",
    "- Complements semantic search by finding syntactically similar code\n",
    "- Effective for code that shares tokens despite different semantics\n",
    "\n",
    "Tokenization Strategy:\n",
    "- Lowercase normalization\n",
    "- Splits on non-alphanumeric characters (preserves underscores in identifiers)\n",
    "- Simple but effective for code tokens\n",
    "\n",
    "Will be combined with FAISS embeddings in detect_hybrid_rag() using score fusion.\n",
    "\n",
    "Output Files:\n",
    "- indexes/bm25_index.pkl (BM25 model)\n",
    "- indexes/tokenized_corpus.pkl (tokenized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BM25 index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c15bb9659147ec94e86a05735f3444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì BM25 index built and saved\n"
     ]
    }
   ],
   "source": [
    "def tokenize_code(code: str) -> List[str]:\n",
    "    \"\"\"Tokenize code for BM25 indexing.\"\"\"\n",
    "    # Split on non-alphanumeric characters but keep underscores\n",
    "    tokens = re.findall(r'\\b\\w+\\b', code.lower())\n",
    "    return tokens\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "tokenized_corpus = [tokenize_code(func['code']) for func in tqdm(all_functions)]\n",
    "bm25_index = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Save BM25 index\n",
    "with open(INDEX_DIR / 'bm25_index.pkl', 'wb') as f:\n",
    "    pickle.dump(bm25_index, f)\n",
    "with open(INDEX_DIR / 'tokenized_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_corpus, f)\n",
    "\n",
    "print(\"‚úì BM25 index built and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Test Dataset with Plagiarism Examples\n",
    "POSITIVE EXAMPLES (15 cases - Plagiarism):\n",
    "Realistic transformations applied to actual repository functions:\n",
    "\n",
    "1. Variable Renaming (5 cases):\n",
    "   - Changes: data‚Üíinput_data, result‚Üíoutput, temp‚Üítmp_var, etc.\n",
    "   - Rationale: Common plagiarism tactic to avoid detection\n",
    "\n",
    "2. Comment/Docstring Removal (5 cases):\n",
    "   - Strips all comments and docstrings\n",
    "   - Rationale: Students often remove documentation from copied code\n",
    "\n",
    "3. Minor Refactoring (5 cases):\n",
    "   - Combines variable renaming + comment removal + whitespace changes\n",
    "   - Rationale: More sophisticated plagiarism attempts\n",
    "\n",
    "NEGATIVE EXAMPLES (15 cases - Non-Plagiarism):\n",
    "- Pairs of functions from DIFFERENT repositories\n",
    "- Ensures functions solve unrelated problems (different domains)\n",
    "- Rationale: True negatives should have fundamentally different implementations\n",
    "\n",
    "Dataset Properties:\n",
    "- Balanced: 50% positive, 50% negative\n",
    "- Labeled: Each case has 'label' field (1=plagiarism, 0=original)\n",
    "- Traceable: 'original_id' links to source function in corpus\n",
    "- Documented: 'transformation' field records modification type\n",
    "\n",
    "Output: data/test_dataset.json (30 test cases)\n",
    "\n",
    "This dataset will be used for evaluation in 03_evaluation.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 283 substantial functions for test dataset\n",
      "‚úì Created 15 positive examples\n",
      "‚úì Created 15 negative examples\n",
      "\n",
      "‚úì Test dataset created with 30 examples\n",
      "  - Positive (plagiarized): 15\n",
      "  - Negative (original): 15\n",
      "‚úì Saved to data\\test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def apply_variable_renaming(code: str) -> str:\n",
    "    \"\"\"Rename variables in code to simulate plagiarism.\"\"\"\n",
    "    # Simple renaming strategy\n",
    "    replacements = {\n",
    "        'data': 'input_data',\n",
    "        'result': 'output',\n",
    "        'temp': 'tmp_var',\n",
    "        'value': 'val',\n",
    "        'item': 'element',\n",
    "        'index': 'idx',\n",
    "        'count': 'counter',\n",
    "        'list': 'lst',\n",
    "        'dict': 'dct',\n",
    "    }\n",
    "    \n",
    "    modified = code\n",
    "    for old, new in replacements.items():\n",
    "        modified = re.sub(r'\\b' + old + r'\\b', new, modified)\n",
    "    \n",
    "    return modified\n",
    "\n",
    "def remove_comments_and_docstrings(code: str) -> str:\n",
    "    \"\"\"Remove comments and docstrings from code.\"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_docstring = False\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Toggle docstring state\n",
    "        if '\"\"\"' in stripped or \"'''\" in stripped:\n",
    "            in_docstring = not in_docstring\n",
    "            continue\n",
    "        \n",
    "        # Skip lines in docstrings or comments\n",
    "        if in_docstring or stripped.startswith('#'):\n",
    "            continue\n",
    "        \n",
    "        # Remove inline comments\n",
    "        if '#' in line:\n",
    "            line = line[:line.index('#')]\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def add_extra_whitespace(code: str) -> str:\n",
    "    \"\"\"Add extra whitespace to code.\"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    modified_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        modified_lines.append(line)\n",
    "        if random.random() < 0.2:  # 20% chance to add blank line\n",
    "            modified_lines.append('')\n",
    "    \n",
    "    return '\\n'.join(modified_lines)\n",
    "\n",
    "def minor_refactoring(code: str) -> str:\n",
    "    \"\"\"Apply minor refactoring changes.\"\"\"\n",
    "    # Combine multiple transformations\n",
    "    code = apply_variable_renaming(code)\n",
    "    code = remove_comments_and_docstrings(code)\n",
    "    code = add_extra_whitespace(code)\n",
    "    return code\n",
    "\n",
    "# Select functions for test cases\n",
    "# Filter to get substantial functions\n",
    "substantial_functions = [f for f in all_functions if len(f['code']) > 200]\n",
    "print(f\"Selected {len(substantial_functions)} substantial functions for test dataset\")\n",
    "\n",
    "# Create positive examples (plagiarized)\n",
    "num_positive = 15\n",
    "positive_examples = []\n",
    "\n",
    "selected_originals = random.sample(substantial_functions, num_positive)\n",
    "\n",
    "for i, original in enumerate(selected_originals):\n",
    "    # Apply different transformation types\n",
    "    transformation_type = i % 3\n",
    "    \n",
    "    if transformation_type == 0:\n",
    "        plagiarized_code = apply_variable_renaming(original['code'])\n",
    "        transform_desc = \"variable_renaming\"\n",
    "    elif transformation_type == 1:\n",
    "        plagiarized_code = remove_comments_and_docstrings(original['code'])\n",
    "        transform_desc = \"comment_removal\"\n",
    "    else:\n",
    "        plagiarized_code = minor_refactoring(original['code'])\n",
    "        transform_desc = \"minor_refactoring\"\n",
    "    \n",
    "    positive_examples.append({\n",
    "        'id': f\"pos_{i}\",\n",
    "        'query_code': plagiarized_code,\n",
    "        'original_id': original['id'],\n",
    "        'label': 1,  # Plagiarized\n",
    "        'transformation': transform_desc\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Created {len(positive_examples)} positive examples\")\n",
    "\n",
    "# Create negative examples (original code)\n",
    "num_negative = 15\n",
    "negative_examples = []\n",
    "\n",
    "# Negative examples: functions from unrelated domains\n",
    "for i in range(num_negative):\n",
    "    # Select two random functions from different repositories\n",
    "    func1, func2 = random.sample(substantial_functions, 2)\n",
    "    \n",
    "    while func1['repo'] == func2['repo']:\n",
    "        func1, func2 = random.sample(substantial_functions, 2)\n",
    "    \n",
    "    negative_examples.append({\n",
    "        'id': f\"neg_{i}\",\n",
    "        'query_code': func1['code'],\n",
    "        'original_id': func2['id'],  # Different function\n",
    "        'label': 0,  # Not plagiarized\n",
    "        'transformation': 'none'\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Created {len(negative_examples)} negative examples\")\n",
    "\n",
    "# Combine and shuffle\n",
    "test_dataset = positive_examples + negative_examples\n",
    "random.shuffle(test_dataset)\n",
    "\n",
    "# Save test dataset\n",
    "test_file = DATA_DIR / 'test_dataset.json'\n",
    "with open(test_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_dataset, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Test dataset created with {len(test_dataset)} examples\")\n",
    "print(f\"  - Positive (plagiarized): {len(positive_examples)}\")\n",
    "print(f\"  - Negative (original): {len(negative_examples)}\")\n",
    "print(f\"‚úì Saved to {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Index Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INDEXING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìÅ Data Collection:\n",
      "  - Repositories: 5\n",
      "  - Files downloaded: 150\n",
      "  - Functions extracted: 399\n",
      "\n",
      "üîç Indexes Built:\n",
      "  - FAISS embedding index: 399 vectors\n",
      "  - BM25 lexical index: 399 documents\n",
      "\n",
      "üìä Test Dataset:\n",
      "  - Total test cases: 30\n",
      "  - Positive examples: 15\n",
      "  - Negative examples: 15\n",
      "\n",
      "üíæ Files Saved:\n",
      "  - data\\reference_corpus/raw_corpus.json\n",
      "  - data\\reference_corpus/functions_corpus.json\n",
      "  - indexes/faiss_index.bin\n",
      "  - indexes/embeddings.npy\n",
      "  - indexes/function_metadata.pkl\n",
      "  - indexes/bm25_index.pkl\n",
      "  - indexes/tokenized_corpus.pkl\n",
      "  - data/test_dataset.json\n",
      "\n",
      "‚úÖ All indexing tasks completed successfully!\n",
      "üìù Ready for interactive testing (02_interactive.ipynb)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INDEXING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÅ Data Collection:\")\n",
    "print(f\"  - Repositories: {len(REPOSITORIES)}\")\n",
    "print(f\"  - Files downloaded: {len(all_files)}\")\n",
    "print(f\"  - Functions extracted: {len(all_functions)}\")\n",
    "\n",
    "print(f\"\\nüîç Indexes Built:\")\n",
    "print(f\"  - FAISS embedding index: {index.ntotal} vectors\")\n",
    "print(f\"  - BM25 lexical index: {len(tokenized_corpus)} documents\")\n",
    "\n",
    "print(f\"\\nüìä Test Dataset:\")\n",
    "print(f\"  - Total test cases: {len(test_dataset)}\")\n",
    "print(f\"  - Positive examples: {len(positive_examples)}\")\n",
    "print(f\"  - Negative examples: {len(negative_examples)}\")\n",
    "\n",
    "print(f\"\\nüíæ Files Saved:\")\n",
    "print(f\"  - {CORPUS_DIR}/raw_corpus.json\")\n",
    "print(f\"  - {CORPUS_DIR}/functions_corpus.json\")\n",
    "print(f\"  - {INDEX_DIR}/faiss_index.bin\")\n",
    "print(f\"  - {INDEX_DIR}/embeddings.npy\")\n",
    "print(f\"  - {INDEX_DIR}/function_metadata.pkl\")\n",
    "print(f\"  - {INDEX_DIR}/bm25_index.pkl\")\n",
    "print(f\"  - {INDEX_DIR}/tokenized_corpus.pkl\")\n",
    "print(f\"  - {DATA_DIR}/test_dataset.json\")\n",
    "\n",
    "print(\"\\n‚úÖ All indexing tasks completed successfully!\")\n",
    "print(\"üìù Ready for interactive testing (02_interactive.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
