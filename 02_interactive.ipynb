{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Plagiarism Detection - Phase 2: Interactive Testing\n",
    "This notebook provides interactive functions to test plagiarism detection using four different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Embeddings and search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "# LLM - Gemini\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Setup\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "INDEX_DIR = BASE_DIR / 'indexes'\n",
    "\n",
    "# API key from environment\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "client = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-built Indexes\n",
    "This cell loads all indexes created by 01_indexing.ipynb:\n",
    "\n",
    "1. FAISS Index (faiss_index.bin)\n",
    "   - 399 function embeddings\n",
    "   - Used by: detect_embedding(), detect_rag(), detect_hybrid_rag()\n",
    "\n",
    "2. Function Metadata (function_metadata.pkl)\n",
    "   - Function names, code, docstrings, repository info\n",
    "   - Maps index positions to actual code\n",
    "\n",
    "3. BM25 Index (bm25_index.pkl)\n",
    "   - Lexical matching index\n",
    "   - Used by: detect_hybrid_rag()\n",
    "\n",
    "4. CodeBERT Model (microsoft/codebert-base)\n",
    "   - Same model used in Phase 1 for consistent embeddings\n",
    "   - Used to encode query code\n",
    "\n",
    "Does not re-index; loads existing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indexes and metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded FAISS index with 399 vectors\n",
      "✓ Loaded 399 function metadata entries\n",
      "✓ Loaded BM25 index with 399 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading indexes and metadata...\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('microsoft/codebert-base')\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(str(INDEX_DIR / 'faiss_index.bin'))\n",
    "embeddings = np.load(INDEX_DIR / 'embeddings.npy')\n",
    "\n",
    "# Load function metadata\n",
    "with open(INDEX_DIR / 'function_metadata.pkl', 'rb') as f:\n",
    "    function_metadata = pickle.load(f)\n",
    "\n",
    "# Load BM25 index\n",
    "with open(INDEX_DIR / 'bm25_index.pkl', 'rb') as f:\n",
    "    bm25_index = pickle.load(f)\n",
    "\n",
    "with open(INDEX_DIR / 'tokenized_corpus.pkl', 'rb') as f:\n",
    "    tokenized_corpus = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded FAISS index with {faiss_index.ntotal} vectors\")\n",
    "print(f\"✓ Loaded {len(function_metadata)} function metadata entries\")\n",
    "print(f\"✓ Loaded BM25 index with {len(tokenized_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "These functions support the four main detection methods:\n",
    "\n",
    "1. tokenize_code()\n",
    "   - Tokenizes code for BM25 lexical matching\n",
    "   - Keeps underscores in identifiers (variable_name stays intact)\n",
    "\n",
    "2. retrieve_with_embeddings()\n",
    "   - Semantic search using CodeBERT embeddings\n",
    "   - Returns top-k similar functions with cosine similarity scores\n",
    "   - Used by: detect_embedding(), detect_rag()\n",
    "\n",
    "3. retrieve_with_bm25()\n",
    "   - Lexical search using BM25 algorithm\n",
    "   - Returns top-k functions based on keyword overlap\n",
    "   - Used by: detect_hybrid_rag()\n",
    "\n",
    "4. hybrid_retrieve()\n",
    "   - Combines embeddings + BM25 with weighted fusion\n",
    "   - alpha parameter controls weight (default 0.5 = equal weight)\n",
    "   - Normalizes and fuses scores from both methods\n",
    "   - Used by: detect_hybrid_rag()\n",
    "\n",
    "5. call_llm()\n",
    "   - Wrapper for Gemini API calls\n",
    "   - Adds system instruction for consistent LLM behavior\n",
    "   - Error handling for API failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def tokenize_code(code: str) -> List[str]:\n",
    "    \"\"\"Tokenize code for BM25.\"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', code.lower())\n",
    "    return tokens\n",
    "\n",
    "def retrieve_with_embeddings(query_code: str, k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar functions using embedding-based search.\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to search for\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (function_metadata, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query_code])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    similarities, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx, sim in zip(indices[0], similarities[0]):\n",
    "        results.append((function_metadata[idx], float(sim)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def retrieve_with_bm25(query_code: str, k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar functions using BM25 lexical search.\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to search for\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (function_metadata, bm25_score) tuples\n",
    "    \"\"\"\n",
    "    # Tokenize query\n",
    "    query_tokens = tokenize_code(query_code)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25_index.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx in top_k_indices:\n",
    "        results.append((function_metadata[idx], float(scores[idx])))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def hybrid_retrieve(query_code: str, k: int = 5, alpha: float = 0.5) -> List[Tuple[Dict, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve using hybrid approach combining embeddings and BM25.\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to search for\n",
    "        k: Number of results to return\n",
    "        alpha: Weight for embedding scores (1-alpha for BM25)\n",
    "    \n",
    "    Returns:\n",
    "        List of (function_metadata, combined_score) tuples\n",
    "    \"\"\"\n",
    "    # Get results from both methods\n",
    "    embedding_results = retrieve_with_embeddings(query_code, k=k*2)\n",
    "    bm25_results = retrieve_with_bm25(query_code, k=k*2)\n",
    "    \n",
    "    # Normalize scores\n",
    "    def normalize_scores(results):\n",
    "        scores = [score for _, score in results]\n",
    "        if max(scores) > min(scores):\n",
    "            normalized = [(meta, (score - min(scores)) / (max(scores) - min(scores))) \n",
    "                         for meta, score in results]\n",
    "        else:\n",
    "            normalized = [(meta, 1.0) for meta, _ in results]\n",
    "        return normalized\n",
    "    \n",
    "    embedding_results = normalize_scores(embedding_results)\n",
    "    bm25_results = normalize_scores(bm25_results)\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = {}\n",
    "    \n",
    "    for meta, score in embedding_results:\n",
    "        func_id = meta['id']\n",
    "        combined_scores[func_id] = combined_scores.get(func_id, 0) + alpha * score\n",
    "    \n",
    "    for meta, score in bm25_results:\n",
    "        func_id = meta['id']\n",
    "        combined_scores[func_id] = combined_scores.get(func_id, 0) + (1 - alpha) * score\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_ids = sorted(combined_scores.keys(), key=lambda x: combined_scores[x], reverse=True)\n",
    "    \n",
    "    # Get metadata for top-k\n",
    "    results = []\n",
    "    for func_id in sorted_ids[:k]:\n",
    "        meta = next(f for f in function_metadata if f['id'] == func_id)\n",
    "        results.append((meta, combined_scores[func_id]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def call_llm(prompt: str, max_tokens: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Call Gemini API with prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        max_tokens: Maximum tokens in response (not used by Gemini)\n",
    "    \n",
    "    Returns:\n",
    "        LLM response text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add system instruction to the beginning of prompt\n",
    "        full_prompt = \"You are an expert code analyst specializing in plagiarism detection.\\n\\n\" + prompt\n",
    "        response = client.generate_content(full_prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error calling LLM: {str(e)}\"\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Pure Embedding Search\n",
    "Implementation Strategy:\n",
    "- Uses ONLY embeddings (no LLM)\n",
    "- Combines multiple signals for robust detection:\n",
    "  1. Embedding similarity (semantic understanding)\n",
    "  2. Lexical similarity (exact text matching via SequenceMatcher)\n",
    "  3. Code length ratio (filters false positives)\n",
    "\n",
    "Detection Logic:\n",
    "- Early rejection: Embedding score < 0.88 → not plagiarism\n",
    "- High lexical match (>70%) → definitely plagiarism\n",
    "- High embedding + moderate lexical + similar length → plagiarism\n",
    "- Filters trivial code (<5 lines)\n",
    "\n",
    "Threshold Selection Rationale:\n",
    "- 0.85-0.95 embedding threshold range (tuned for code similarity)\n",
    "- Lexical similarity helps catch variable-renamed plagiarism\n",
    "- Length ratio prevents matching snippets from different-sized functions\n",
    "\n",
    "Returns:\n",
    "- is_plagiarized: Boolean\n",
    "- confidence: 0.0-1.0 score\n",
    "- best_match: Top matching function from corpus\n",
    "- top_matches: Top 3 candidates for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example - Pure Embedding Search:\n",
      "Is Plagiarized: True\n",
      "Confidence: 0.738\n",
      "Best Match: base_check\n"
     ]
    }
   ],
   "source": [
    "def detect_embedding(query_code: str, threshold: float = 0.85, k: int = 5) -> Dict:\n",
    "    \"\"\"\n",
    "    Fixed embedding detection with proper false positive filtering.\n",
    "    \"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    results = retrieve_with_embeddings(query_code, k=k)\n",
    "    \n",
    "    if not results:\n",
    "        return _negative_result()\n",
    "    \n",
    "    best_match, embedding_score = results[0]\n",
    "    \n",
    "    # ⭐ EARLY REJECTION - if best match is too dissimilar, stop here\n",
    "    if embedding_score < 0.88:  # Adjust this threshold based on your data\n",
    "        return _negative_result()\n",
    "    \n",
    "    lexical_similarity = SequenceMatcher(None, query_code, best_match['code']).ratio()\n",
    "    query_lines = len([l for l in query_code.split('\\n') if l.strip()])\n",
    "    match_lines = len([l for l in best_match['code'].split('\\n') if l.strip()])\n",
    "    \n",
    "    # Trivial code filter\n",
    "    if query_lines < 5:\n",
    "        return _negative_result()\n",
    "    \n",
    "    # High lexical = plagiarism\n",
    "    if lexical_similarity > 0.7:\n",
    "        return _positive_result(best_match, lexical_similarity, results)\n",
    "    \n",
    "    # High embedding + moderate lexical + similar length\n",
    "    if embedding_score > 0.95 and lexical_similarity > 0.4:\n",
    "        line_ratio = min(query_lines, match_lines) / max(query_lines, match_lines)\n",
    "        if line_ratio > 0.7:\n",
    "            confidence = (embedding_score * 0.5 + lexical_similarity * 0.5)\n",
    "            return _positive_result(best_match, confidence, results)\n",
    "    \n",
    "    # Medium-high both scores\n",
    "    if embedding_score > 0.92 and lexical_similarity > 0.5:\n",
    "        confidence = (lexical_similarity * 0.6 + embedding_score * 0.4)\n",
    "        return _positive_result(best_match, confidence, results)\n",
    "    \n",
    "    # Default: not plagiarism\n",
    "    return _negative_result()\n",
    "\n",
    "\n",
    "def _negative_result():\n",
    "    return {\n",
    "        'method': 'embedding_search',\n",
    "        'is_plagiarized': False,\n",
    "        'confidence': 0.0,\n",
    "        'best_match': {'id': None, 'name': None, 'repo': None},  # ✅ Changed from None to dict\n",
    "        'top_matches': []\n",
    "    }\n",
    "\n",
    "\n",
    "def _positive_result(best_match, confidence, results):\n",
    "    return {\n",
    "        'method': 'embedding_search',\n",
    "        'is_plagiarized': True,\n",
    "        'confidence': confidence,\n",
    "        'best_match': {\n",
    "            'id': best_match['id'],\n",
    "            'name': best_match['name'],\n",
    "            'repo': best_match['repo']\n",
    "        },\n",
    "        'top_matches': [\n",
    "            {'id': meta['id'], 'name': meta['name'], 'similarity': score}\n",
    "            for meta, score in results[:3]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Test example - UPDATE THIS TOO\n",
    "test_code = \"\"\"\n",
    "def calculate_sum(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "result = detect_embedding(test_code)\n",
    "print(\"\\nExample - Pure Embedding Search:\")\n",
    "print(f\"Is Plagiarized: {result['is_plagiarized']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "\n",
    "# ✅ Safely access best_match\n",
    "best_match = result['best_match']\n",
    "if best_match and best_match.get('name'):\n",
    "    print(f\"Best Match: {best_match['name']}\")\n",
    "else:\n",
    "    print(f\"Best Match: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Direct LLM Analysis\n",
    "Implementation Strategy:\n",
    "- Provides full context (up to 20 reference functions) to LLM\n",
    "- LLM analyzes with complete information access\n",
    "- Tests what LLMs can achieve with full context vs. limited retrieval\n",
    "\n",
    "Process:\n",
    "1. Pre-retrieve top 20 candidates (reduces context to fit token limits)\n",
    "2. Build context with reference functions\n",
    "3. Ask LLM to determine plagiarism with structured output format\n",
    "4. Parse response into structured result\n",
    "\n",
    "Prompt Engineering:\n",
    "- Instructs LLM to consider variable renaming, comment removal, refactoring\n",
    "- Requires specific output format for consistent parsing:\n",
    "  PLAGIARIZED: YES/NO\n",
    "  CONFIDENCE: 0.0-1.0\n",
    "  MATCH: function number\n",
    "  REASON: explanation\n",
    "\n",
    "Robust Parsing:\n",
    "- Handles LLM output variations\n",
    "- Fallback to default values if parsing fails\n",
    "- Normalizes confidence scores (handles 0-1 or 0-100 scales)\n",
    "\n",
    "Trade-offs:\n",
    "- ✅ Pros: Contextual understanding, catches subtle similarities\n",
    "- ❌ Cons: Token limits restrict full corpus, slower, API costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example - Direct LLM Analysis:\n",
      "Is Plagiarized: False\n",
      "Confidence: 1.000\n",
      "Reason: The query code calculates the sum of numbers in a list, which is a very fundamental and common progr...\n"
     ]
    }
   ],
   "source": [
    "def detect_llm(query_code: str, max_context_functions: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect plagiarism using direct LLM analysis with full context.\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to check\n",
    "        max_context_functions: Maximum number of reference functions to include\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing detection results\n",
    "    \"\"\"\n",
    "    # First, get most relevant functions to reduce context size\n",
    "    candidates = retrieve_with_embeddings(query_code, k=max_context_functions)\n",
    "    \n",
    "    if not candidates:\n",
    "        return {\n",
    "            'method': 'direct_llm',\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'best_match': {'id': None, 'name': None, 'repo': None},\n",
    "            'reason': 'No candidate functions retrieved',\n",
    "            'raw_response': ''\n",
    "        }\n",
    "    \n",
    "    # Build context with reference functions\n",
    "    context = \"Reference code functions:\\n\\n\"\n",
    "    for i, (meta, _) in enumerate(candidates[:max_context_functions]):\n",
    "        context += f\"Function {i+1}: {meta['name']}\\n\"\n",
    "        context += f\"From: {meta['repo']}\\n\"\n",
    "        context += f\"{meta['code'][:500]}\\n\\n\"  # Limit length\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"You are an expert code analyst specializing in plagiarism detection.\n",
    "\n",
    "{context}\n",
    "\n",
    "Query code to analyze:\n",
    "{query_code}\n",
    "\n",
    "Analyze if the query code is plagiarized from any of the reference functions.\n",
    "Consider variable renaming, comment removal, and minor refactoring as signs of plagiarism.\n",
    "\n",
    "Respond in this exact format:\n",
    "PLAGIARIZED: [YES/NO]\n",
    "CONFIDENCE: [0.0-1.0]\n",
    "MATCH: [function number or NONE]\n",
    "REASON: [brief explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    response = call_llm(prompt, max_tokens=300)\n",
    "    \n",
    "    # Default values\n",
    "    is_plagiarized = False\n",
    "    confidence = 0.5\n",
    "    best_match = candidates[0][0] if candidates else None\n",
    "    reason = \"Unable to parse response\"\n",
    "    \n",
    "    # Try to parse response - NOW WITH PROPER ERROR HANDLING\n",
    "    try:\n",
    "        if 'PLAGIARIZED:' in response:\n",
    "            plagiarized_part = response.split('PLAGIARIZED:')[1].split('\\n')[0].upper()\n",
    "            is_plagiarized = 'YES' in plagiarized_part\n",
    "        \n",
    "        if 'CONFIDENCE:' in response:\n",
    "            confidence_str = response.split('CONFIDENCE:')[1].split('\\n')[0].strip()\n",
    "            conf_match = re.findall(r'\\d+\\.?\\d*', confidence_str)\n",
    "            if conf_match:\n",
    "                confidence = float(conf_match[0])\n",
    "                if confidence > 1.0:\n",
    "                    confidence = confidence / 100.0\n",
    "        \n",
    "        if 'MATCH:' in response:\n",
    "            match_str = response.split('MATCH:')[1].split('\\n')[0].strip()\n",
    "            if 'NONE' not in match_str.upper():\n",
    "                match_nums = re.findall(r'\\d+', match_str)\n",
    "                if match_nums:\n",
    "                    match_num = int(match_nums[0])\n",
    "                    if 1 <= match_num <= len(candidates):\n",
    "                        best_match = candidates[match_num - 1][0]\n",
    "        \n",
    "        if 'REASON:' in response:\n",
    "            reason = response.split('REASON:')[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Error parsing LLM response: {str(e)}\")\n",
    "        print(f\"    Raw response: {response[:200]}...\")\n",
    "    \n",
    "    # Ensure best_match is not None\n",
    "    if best_match is None and candidates:\n",
    "        best_match = candidates[0][0]\n",
    "    \n",
    "    return {\n",
    "        'method': 'direct_llm',\n",
    "        'is_plagiarized': is_plagiarized,\n",
    "        'confidence': confidence,\n",
    "        'best_match': {\n",
    "            'id': best_match['id'] if best_match else None,\n",
    "            'name': best_match['name'] if best_match else None,\n",
    "            'repo': best_match['repo'] if best_match else None\n",
    "        } if best_match else {'id': None, 'name': None, 'repo': None},\n",
    "        'reason': reason,\n",
    "        'raw_response': response\n",
    "    }\n",
    "\n",
    "# Test example\n",
    "result = detect_llm(test_code)\n",
    "print(\"\\nExample - Direct LLM Analysis:\")\n",
    "print(f\"Is Plagiarized: {result['is_plagiarized']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "print(f\"Reason: {result['reason'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Standard RAG\n",
    "Retrieval-Augmented Generation Architecture:\n",
    "1. Retrieval Stage: Use embeddings to find top-k relevant functions\n",
    "2. Augmentation Stage: Build context with retrieved code\n",
    "3. Generation Stage: LLM analyzes query against retrieved context\n",
    "\n",
    "Key Differences from Method 2:\n",
    "- Fewer retrieved documents (k=5 vs. 20)\n",
    "- More focused context → better LLM performance\n",
    "- Standard RAG pattern: retrieve first, then reason\n",
    "\n",
    "Prompt Design:\n",
    "- Explicit instructions to look for:\n",
    "  • Identical logic with cosmetic changes\n",
    "  • Variable/function renaming\n",
    "  • Comment removal\n",
    "  • Whitespace modifications\n",
    "  \n",
    "- Structured output format (same as Method 2)\n",
    "\n",
    "Advantages over Pure Embedding:\n",
    "- LLM understands semantic equivalence despite syntactic differences\n",
    "- Can explain WHY code is similar (interpretability)\n",
    "\n",
    "Advantages over Direct LLM:\n",
    "- Focused context improves accuracy\n",
    "- Scalable to larger corpora (retrieval filters irrelevant code)\n",
    "- Lower token usage → faster + cheaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example - Standard RAG:\n",
      "Is Plagiarized: False\n",
      "Confidence: 1.000\n",
      "Retrieved: 5 functions\n"
     ]
    }
   ],
   "source": [
    "def detect_rag(query_code: str, k: int = 5) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect plagiarism using standard RAG (Retrieval-Augmented Generation).\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to check\n",
    "        k: Number of functions to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing detection results\n",
    "    \"\"\"\n",
    "    # Retrieve relevant functions using embeddings\n",
    "    retrieved = retrieve_with_embeddings(query_code, k=k)\n",
    "    \n",
    "    if not retrieved:\n",
    "        return {\n",
    "            'method': 'standard_rag',\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'best_match': {'id': None, 'name': None, 'repo': None},\n",
    "            'retrieved_count': 0,\n",
    "            'reason': 'No functions retrieved',\n",
    "            'raw_response': ''\n",
    "        }\n",
    "    \n",
    "    # Build context\n",
    "    context = \"Retrieved reference functions:\\n\\n\"\n",
    "    for i, (meta, score) in enumerate(retrieved):\n",
    "        context += f\"Function {i+1}: {meta['name']} (similarity: {score:.3f})\\n\"\n",
    "        context += f\"Repository: {meta['repo']}\\n\"\n",
    "        context += f\"Code:\\n{meta['code']}\\n\\n\"\n",
    "    \n",
    "    # Create prompt with system instruction included\n",
    "    prompt = f\"\"\"You are an expert code analyst specializing in plagiarism detection.\n",
    "\n",
    "{context}\n",
    "\n",
    "Query code to analyze:\n",
    "{query_code}\n",
    "\n",
    "Determine if the query code is plagiarized from any of the retrieved functions.\n",
    "Look for identical logic despite superficial changes like:\n",
    "- Variable/function renaming\n",
    "- Comment/docstring removal\n",
    "- Whitespace changes\n",
    "- Minor reordering that preserves logic\n",
    "\n",
    "Respond in this exact format:\n",
    "PLAGIARIZED: [YES/NO]\n",
    "CONFIDENCE: [0.0-1.0]\n",
    "MATCH: [function number or NONE]\n",
    "REASON: [brief explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    response = call_llm(prompt, max_tokens=300)\n",
    "    \n",
    "    # Default values - SET THESE FIRST!\n",
    "    is_plagiarized = False\n",
    "    confidence = 0.0\n",
    "    best_match = retrieved[0][0] if retrieved else None\n",
    "    reason = \"Unable to parse response\"\n",
    "    \n",
    "    # Try to parse response - WITH PROPER ERROR HANDLING\n",
    "    try:\n",
    "        if 'PLAGIARIZED:' in response:\n",
    "            plagiarized_part = response.split('PLAGIARIZED:')[1].split('\\n')[0].upper()\n",
    "            is_plagiarized = 'YES' in plagiarized_part\n",
    "        \n",
    "        if 'CONFIDENCE:' in response:\n",
    "            confidence_str = response.split('CONFIDENCE:')[1].split('\\n')[0].strip()\n",
    "            conf_match = re.findall(r'\\d+\\.?\\d*', confidence_str)\n",
    "            if conf_match:\n",
    "                confidence = float(conf_match[0])\n",
    "                # If confidence is given as percentage (e.g., 85), convert to 0-1 scale\n",
    "                if confidence > 1.0:\n",
    "                    confidence = confidence / 100.0\n",
    "        else:\n",
    "            # Fallback to similarity score\n",
    "            confidence = retrieved[0][1] if retrieved else 0.0\n",
    "        \n",
    "        if 'MATCH:' in response:\n",
    "            match_str = response.split('MATCH:')[1].split('\\n')[0].strip()\n",
    "            if 'NONE' not in match_str.upper():\n",
    "                match_nums = re.findall(r'\\d+', match_str)\n",
    "                if match_nums:\n",
    "                    match_num = int(match_nums[0])\n",
    "                    if 1 <= match_num <= len(retrieved):\n",
    "                        best_match = retrieved[match_num - 1][0]\n",
    "        \n",
    "        if 'REASON:' in response:\n",
    "            reason = response.split('REASON:')[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Error parsing LLM response: {str(e)}\")\n",
    "        print(f\"    Raw response: {response[:200]}...\")\n",
    "        # Keep default values\n",
    "    \n",
    "    # Ensure best_match is not None\n",
    "    if best_match is None and retrieved:\n",
    "        best_match = retrieved[0][0]\n",
    "    \n",
    "    return {\n",
    "        'method': 'standard_rag',\n",
    "        'is_plagiarized': is_plagiarized,\n",
    "        'confidence': confidence,\n",
    "        'best_match': {\n",
    "            'id': best_match['id'] if best_match else None,\n",
    "            'name': best_match['name'] if best_match else None,\n",
    "            'repo': best_match['repo'] if best_match else None\n",
    "        } if best_match else {'id': None, 'name': None, 'repo': None},\n",
    "        'retrieved_count': len(retrieved),\n",
    "        'reason': reason,\n",
    "        'raw_response': response\n",
    "    }\n",
    "# Test example\n",
    "result = detect_rag(test_code)\n",
    "print(\"\\nExample - Standard RAG:\")\n",
    "print(f\"Is Plagiarized: {result['is_plagiarized']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "print(f\"Retrieved: {result['retrieved_count']} functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Hybrid RAG\n",
    "Hybrid Retrieval Architecture:\n",
    "- Combines dense (embeddings) + sparse (BM25) retrieval\n",
    "- Fusion strategy: Weighted score combination (alpha parameter)\n",
    "\n",
    "Why Hybrid?\n",
    "1. Embeddings capture semantic similarity\n",
    "2. BM25 captures lexical/keyword matching\n",
    "3. Together: More robust than either alone\n",
    "\n",
    "Fusion Process:\n",
    "1. Retrieve with embeddings → get top 2k results\n",
    "2. Retrieve with BM25 → get top 2k results\n",
    "3. Normalize scores to [0,1] range\n",
    "4. Combine: score = alpha * embedding + (1-alpha) * bm25\n",
    "5. Rank by combined score → select top k\n",
    "\n",
    "Alpha Parameter (default=0.5):\n",
    "- 0.5: Equal weight to semantic + lexical\n",
    "- >0.5: Favor semantic similarity\n",
    "- <0.5: Favor keyword matching\n",
    "- Will be ablated in evaluation (Phase 3)\n",
    "\n",
    "Robust LLM Response Parsing:\n",
    "- Primary: Structured format parsing\n",
    "- Fallback: Keyword-based confidence estimation\n",
    "- Handles malformed responses gracefully\n",
    "\n",
    "When Hybrid Helps:\n",
    "- Plagiarism with variable renaming: BM25 catches structural keywords\n",
    "- Semantic refactoring: Embeddings catch logic similarity\n",
    "- Best of both worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM Response]\n",
      "PLAGIARIZED: NO\n",
      "CONFIDENCE: 1.0\n",
      "MATCH: NONE\n",
      "REASON: The query code implements a basic sum accumulation pattern, which is a fundamental programming concept and not a unique or complex algorithm derived from the provided reference functions.\n",
      "\n",
      "\n",
      "✅ Hybrid RAG Result:\n",
      "  Plagiarized: False\n",
      "  Confidence: 1.000\n",
      "  Best Match: base_check\n",
      "  Reason: The query code implements a basic sum accumulation pattern, which is a fundamental programming concept and not a unique or complex algorithm derived from the provided reference functions.\n"
     ]
    }
   ],
   "source": [
    "def detect_hybrid_rag(query_code: str, k: int = 5, alpha: float = 0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect plagiarism using hybrid RAG (dense + sparse retrieval).\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to check\n",
    "        k: Number of functions to retrieve\n",
    "        alpha: Weight for embedding scores (1-alpha for BM25)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing detection results\n",
    "    \"\"\"\n",
    "    # Retrieve using hybrid approach\n",
    "    retrieved = hybrid_retrieve(query_code, k=k, alpha=alpha)\n",
    "    \n",
    "    if not retrieved:\n",
    "        return {\n",
    "            'method': 'hybrid_rag',\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'best_match': None,\n",
    "            'retrieved_count': 0,\n",
    "            'fusion_alpha': alpha,\n",
    "            'reason': 'No similar code found in corpus',\n",
    "            'raw_response': 'No retrieval results'\n",
    "        }\n",
    "    \n",
    "    # Build context\n",
    "    context = \"Retrieved reference functions (hybrid search):\\n\\n\"\n",
    "    for i, (meta, score) in enumerate(retrieved):\n",
    "        context += f\"Function {i+1}: {meta['name']} (score: {score:.3f})\\n\"\n",
    "        context += f\"Repository: {meta['repo']}\\n\"\n",
    "        context += f\"Code:\\n{meta['code'][:500]}...\\n\\n\"  # Limit code length\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"{context}\n",
    "\n",
    "Query code to analyze:\n",
    "{query_code}\n",
    "\n",
    "Determine if the query code is plagiarized from any of the retrieved functions.\n",
    "These functions were retrieved using both semantic similarity and lexical matching.\n",
    "\n",
    "Look for:\n",
    "- Identical or very similar logic flow\n",
    "- Same algorithm/approach with cosmetic changes\n",
    "- Variable renaming but same structure\n",
    "- Comment removal or modification\n",
    "\n",
    "Respond in this EXACT format (no extra text):\n",
    "PLAGIARIZED: YES or NO\n",
    "CONFIDENCE: 0.85\n",
    "MATCH: 1 or NONE\n",
    "REASON: Brief explanation here\n",
    "\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    try:\n",
    "        response = call_llm(prompt, max_tokens=300)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM call failed: {e}\")\n",
    "        response = \"\"\n",
    "    \n",
    "    # Robust parsing with fallbacks\n",
    "    is_plagiarized = False\n",
    "    confidence = 0.0\n",
    "    match_num = None\n",
    "    reason = \"Unable to parse LLM response\"\n",
    "    \n",
    "    # Parse PLAGIARIZED\n",
    "    try:\n",
    "        if 'PLAGIARIZED:' in response:\n",
    "            plagiarized_line = response.split('PLAGIARIZED:')[1].split('\\n')[0].strip().upper()\n",
    "            is_plagiarized = 'YES' in plagiarized_line\n",
    "        else:\n",
    "            # Fallback: look for yes/no in response\n",
    "            response_upper = response.upper()\n",
    "            if 'YES' in response_upper and 'PLAGIARIZED' in response_upper:\n",
    "                is_plagiarized = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error parsing PLAGIARIZED: {e}\")\n",
    "    \n",
    "    # Parse CONFIDENCE\n",
    "    try:\n",
    "        if 'CONFIDENCE:' in response:\n",
    "            confidence_line = response.split('CONFIDENCE:')[1].split('\\n')[0].strip()\n",
    "            # Extract float\n",
    "            conf_match = re.search(r'(\\d+\\.?\\d*)', confidence_line)\n",
    "            if conf_match:\n",
    "                confidence = float(conf_match.group(1))\n",
    "                # Normalize if needed\n",
    "                if confidence > 1.0:\n",
    "                    confidence = confidence / 100.0\n",
    "        else:\n",
    "            # Fallback: use top retrieval score\n",
    "            confidence = retrieved[0][1] if retrieved else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error parsing CONFIDENCE: {e}\")\n",
    "        confidence = retrieved[0][1] if retrieved else 0.0\n",
    "    \n",
    "    # Parse MATCH\n",
    "    try:\n",
    "        if 'MATCH:' in response:\n",
    "            match_line = response.split('MATCH:')[1].split('\\n')[0].strip().upper()\n",
    "            if 'NONE' not in match_line:\n",
    "                match_search = re.search(r'(\\d+)', match_line)\n",
    "                if match_search:\n",
    "                    match_num = int(match_search.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error parsing MATCH: {e}\")\n",
    "    \n",
    "    # Parse REASON\n",
    "    try:\n",
    "        if 'REASON:' in response:\n",
    "            reason = response.split('REASON:')[1].strip()\n",
    "            # Clean up if there are multiple lines\n",
    "            reason = reason.split('\\n')[0] if '\\n' in reason else reason\n",
    "        else:\n",
    "            reason = \"LLM response did not follow expected format\"\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error parsing REASON: {e}\")\n",
    "    \n",
    "    # Determine best match\n",
    "    best_match = None\n",
    "    if match_num and 1 <= match_num <= len(retrieved):\n",
    "        best_match = retrieved[match_num - 1][0]\n",
    "    elif retrieved:\n",
    "        best_match = retrieved[0][0]  # Default to top result\n",
    "    \n",
    "    return {\n",
    "        'method': 'hybrid_rag',\n",
    "        'is_plagiarized': is_plagiarized,\n",
    "        'confidence': confidence,\n",
    "        'best_match': {\n",
    "            'id': best_match['id'] if best_match else None,\n",
    "            'name': best_match['name'] if best_match else None,\n",
    "            'repo': best_match['repo'] if best_match else None,\n",
    "            'code': best_match['code'][:200] + '...' if best_match else None\n",
    "        },\n",
    "        'retrieved_count': len(retrieved),\n",
    "        'fusion_alpha': alpha,\n",
    "        'reason': reason,\n",
    "        'raw_response': response\n",
    "    }\n",
    "\n",
    "\n",
    "# Alternative: More flexible parsing function\n",
    "def parse_llm_response(response: str, retrieved: List, default_confidence: float = 0.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Robustly parse LLM response with multiple fallback strategies.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'is_plagiarized': False,\n",
    "        'confidence': default_confidence,\n",
    "        'match_num': None,\n",
    "        'reason': 'Unable to parse response'\n",
    "    }\n",
    "    \n",
    "    if not response:\n",
    "        return result\n",
    "    \n",
    "    # Strategy 1: Exact format parsing\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith('PLAGIARIZED:'):\n",
    "            result['is_plagiarized'] = 'YES' in line.upper()\n",
    "        \n",
    "        elif line.startswith('CONFIDENCE:'):\n",
    "            try:\n",
    "                nums = re.findall(r'(\\d+\\.?\\d*)', line)\n",
    "                if nums:\n",
    "                    conf = float(nums[0])\n",
    "                    result['confidence'] = conf if conf <= 1.0 else conf / 100.0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        elif line.startswith('MATCH:'):\n",
    "            try:\n",
    "                if 'NONE' not in line.upper():\n",
    "                    nums = re.findall(r'(\\d+)', line)\n",
    "                    if nums:\n",
    "                        result['match_num'] = int(nums[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        elif line.startswith('REASON:'):\n",
    "            result['reason'] = line.replace('REASON:', '').strip()\n",
    "    \n",
    "    # Strategy 2: Keyword-based fallback\n",
    "    if result['confidence'] == default_confidence:\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Look for confidence indicators\n",
    "        if 'very similar' in response_lower or 'identical' in response_lower:\n",
    "            result['confidence'] = 0.9\n",
    "            result['is_plagiarized'] = True\n",
    "        elif 'similar' in response_lower or 'likely' in response_lower:\n",
    "            result['confidence'] = 0.7\n",
    "            result['is_plagiarized'] = True\n",
    "        elif 'somewhat similar' in response_lower:\n",
    "            result['confidence'] = 0.5\n",
    "        elif 'different' in response_lower or 'not plagiarized' in response_lower:\n",
    "            result['confidence'] = 0.2\n",
    "            result['is_plagiarized'] = False\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Updated version using the parser\n",
    "def detect_hybrid_rag_v2(query_code: str, k: int = 5, alpha: float = 0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect plagiarism using hybrid RAG with robust parsing.\n",
    "    \"\"\"\n",
    "    retrieved = hybrid_retrieve(query_code, k=k, alpha=alpha)\n",
    "    \n",
    "    if not retrieved:\n",
    "        return {\n",
    "            'method': 'hybrid_rag',\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'best_match': None,\n",
    "            'retrieved_count': 0,\n",
    "            'fusion_alpha': alpha,\n",
    "            'reason': 'No similar code found',\n",
    "            'raw_response': ''\n",
    "        }\n",
    "    \n",
    "    # Build shorter context to avoid token limits\n",
    "    context = \"Compare the query code with these reference functions:\\n\\n\"\n",
    "    for i, (meta, score) in enumerate(retrieved[:3]):  # Limit to top 3\n",
    "        context += f\"{i+1}. {meta['name']} (similarity: {score:.2f})\\n\"\n",
    "        context += f\"{meta['code'][:300]}...\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"{context}\n",
    "\n",
    "Query code:\n",
    "{query_code[:500]}\n",
    "\n",
    "Is the query code plagiarized from any reference function?\n",
    "\n",
    "Answer in this format:\n",
    "PLAGIARIZED: YES or NO\n",
    "CONFIDENCE: 0.0 to 1.0\n",
    "MATCH: function number or NONE\n",
    "REASON: one sentence explanation\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = call_llm(prompt, max_tokens=200)\n",
    "        print(f\"\\n[LLM Response]\\n{response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM Error: {e}\")\n",
    "        response = \"\"\n",
    "    \n",
    "    # Parse with fallback\n",
    "    parsed = parse_llm_response(\n",
    "        response, \n",
    "        retrieved, \n",
    "        default_confidence=retrieved[0][1]\n",
    "    )\n",
    "    \n",
    "    # Get best match\n",
    "    if parsed['match_num'] and 1 <= parsed['match_num'] <= len(retrieved):\n",
    "        best_match = retrieved[parsed['match_num'] - 1][0]\n",
    "    else:\n",
    "        best_match = retrieved[0][0]\n",
    "    \n",
    "    return {\n",
    "        'method': 'hybrid_rag',\n",
    "        'is_plagiarized': parsed['is_plagiarized'],\n",
    "        'confidence': parsed['confidence'],\n",
    "        'best_match': {\n",
    "            'id': best_match['id'],\n",
    "            'name': best_match['name'],\n",
    "            'repo': best_match['repo'],\n",
    "            'code_preview': best_match['code'][:150] + '...'\n",
    "        },\n",
    "        'retrieved_count': len(retrieved),\n",
    "        'fusion_alpha': alpha,\n",
    "        'reason': parsed['reason'],\n",
    "        'raw_response': response\n",
    "    }\n",
    "\n",
    "\n",
    "# Test with error handling\n",
    "try:\n",
    "    result = detect_hybrid_rag_v2(test_code)\n",
    "    print(\"\\n✅ Hybrid RAG Result:\")\n",
    "    print(f\"  Plagiarized: {result['is_plagiarized']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"  Best Match: {result['best_match']['name']}\")\n",
    "    print(f\"  Reason: {result['reason']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Detection failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing Interface\n",
    "This cell provides a unified interface for testing all methods:\n",
    "\n",
    "compare_all_methods(query_code):\n",
    "- Runs all 4 detection methods sequentially\n",
    "- Returns structured comparison\n",
    "- Prints side-by-side results\n",
    "\n",
    "Individual Method Usage:\n",
    "- detect_embedding(code): Method 1\n",
    "- detect_llm(code): Method 2\n",
    "- detect_rag(code): Method 3\n",
    "- detect_hybrid_rag(code): Method 4\n",
    "\n",
    "Output Format (all methods):\n",
    "{\n",
    "  'method': str,\n",
    "  'is_plagiarized': bool,\n",
    "  'confidence': float,\n",
    "  'best_match': {\n",
    "    'id': str,\n",
    "    'name': str,\n",
    "    'repo': str\n",
    "  },\n",
    "  'reason': str,  # LLM methods only\n",
    "  ...\n",
    "}\n",
    "\n",
    "Requirement Met: All functions callable independently for 03_evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTERACTIVE TESTING READY\n",
      "============================================================\n",
      "\n",
      "Available functions:\n",
      "  - detect_embedding(code)\n",
      "  - detect_llm(code)\n",
      "  - detect_rag(code)\n",
      "  - detect_hybrid_rag(code)\n",
      "  - compare_all_methods(code)\n",
      "\n",
      "Example:\n",
      "  result = detect_rag(\"\"\"your code here\"\"\")\n"
     ]
    }
   ],
   "source": [
    "def compare_all_methods(query_code: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Run all four detection methods and compare results.\n",
    "    \n",
    "    Args:\n",
    "        query_code: Code snippet to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results from all methods\n",
    "    \"\"\"\n",
    "    print(\"Running all detection methods...\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"[1/4] Pure Embedding Search...\")\n",
    "    results['embedding'] = detect_embedding(query_code)\n",
    "    \n",
    "    print(\"[2/4] Direct LLM Analysis...\")\n",
    "    results['llm'] = detect_llm(query_code)\n",
    "    \n",
    "    print(\"[3/4] Standard RAG...\")\n",
    "    results['rag'] = detect_rag(query_code)\n",
    "    \n",
    "    print(\"[4/4] Hybrid RAG...\")\n",
    "    results['hybrid_rag'] = detect_hybrid_rag(query_code)\n",
    "    \n",
    "    print(\"\\n✓ All methods completed\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON OF ALL METHODS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for method_name, result in results.items():\n",
    "        print(f\"\\n{method_name.upper()}:\")\n",
    "        print(f\"  Plagiarized: {result['is_plagiarized']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"  Best Match: {result['best_match']['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE TESTING READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - detect_embedding(code)\")\n",
    "print(\"  - detect_llm(code)\")\n",
    "print(\"  - detect_rag(code)\")\n",
    "print(\"  - detect_hybrid_rag(code)\")\n",
    "print(\"  - compare_all_methods(code)\")\n",
    "print(\"\\nExample:\")\n",
    "print('  result = detect_rag(\"\"\"your code here\"\"\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_code = \"\"\"\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\"\"\"\n",
    "\n",
    "# Test the RAG method\n",
    "result_rag = detect_rag(sample_code)\n",
    "print(result_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all detection methods...\n",
      "\n",
      "[1/4] Pure Embedding Search...\n",
      "[2/4] Direct LLM Analysis...\n",
      "[3/4] Standard RAG...\n",
      "[4/4] Hybrid RAG...\n",
      "\n",
      "[DEBUG] LLM Response:\n",
      "PLAGIARIZED: NO\n",
      "CONFIDENCE: 1.00\n",
      "MATCH: NONE\n",
      "REASON: The query code performs a very basic arithmetic multiplication. None of the reference functions perform multiplication or share any similar logic, algorithm, or structure with the query code. The query code is a fundamental operation that would be independently written.\n",
      "\n",
      "\n",
      "✓ All methods completed\n",
      "\n",
      "============================================================\n",
      "COMPARISON OF ALL METHODS\n",
      "============================================================\n",
      "\n",
      "EMBEDDING:\n",
      "  Plagiarized: False\n",
      "  Confidence: 0.000\n",
      "  Best Match: None\n",
      "\n",
      "LLM:\n",
      "  Plagiarized: False\n",
      "  Confidence: 1.000\n",
      "  Best Match: concat\n",
      "\n",
      "RAG:\n",
      "  Plagiarized: False\n",
      "  Confidence: 1.000\n",
      "  Best Match: concat\n",
      "\n",
      "HYBRID_RAG:\n",
      "  Plagiarized: False\n",
      "  Confidence: 1.000\n",
      "  Best Match: concat\n"
     ]
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "def multiply_numbers(a, b):\n",
    "    return a * b\n",
    "\"\"\"\n",
    "\n",
    "results = compare_all_methods(sample_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
